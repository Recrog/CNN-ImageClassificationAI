# Traffic Signs Classification Project
# HazÄ±r veri seti ile baÅŸlangÄ±Ã§ kodu

import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import cv2

# Veri setinin bulunduÄŸu klasÃ¶r (GerÃ§ek klasÃ¶r adÄ±nÄ±z)
DATA_PATH = "Trafic Signs Preprocssed data/"

def load_preprocessed_data():
    """Ã–nceden iÅŸlenmiÅŸ veri setini yÃ¼kler"""
    print("Veri yÃ¼kleniyor...")
    
    # Ã–nce dosyalarÄ± kontrol et
    import os
    print(f"Mevcut dizin: {os.getcwd()}")
    
    # Dosya yollarÄ±nÄ± kontrol et
    files_to_check = ["train.pickle", "valid.pickle", "test.pickle"]
    for file in files_to_check:
        full_path = DATA_PATH + file
        if os.path.exists(full_path):
            print(f"âœ… {file} bulundu")
        else:
            print(f"âŒ {file} bulunamadÄ±: {full_path}")
    
    # Alternatif yollarÄ± dene
    possible_paths = [
        "Trafic Signs Preprocssed data/",
        "./Trafic Signs Preprocssed data/", 
        "Trafic Signs Preprocssed data\\",
        "Traffic Signs Preprocessed data/",
        "./Traffic Signs Preprocessed data/", 
        "Traffic Signs Preprocessed data\\",
        ""  # DoÄŸrudan ana dizinde
    ]
    
    found_path = None
    for path in possible_paths:
        if os.path.exists(path + "train.pickle"):
            found_path = path
            print(f"âœ… DoÄŸru yol bulundu: {path}")
            break
    
    if found_path is None:
        print("âŒ HiÃ§bir yolda train.pickle bulunamadÄ±!")
        print("Mevcut dosyalar:")
        for item in os.listdir("."):
            print(f"  - {item}")
        return None, None, None, None, None, None
    
    # DoÄŸru yolu kullan
    DATA_PATH_CORRECT = found_path
    
    # EÄŸitim verisi
    with open(DATA_PATH_CORRECT + "train.pickle", "rb") as f:
        train_data = pickle.load(f)
    X_train = train_data['features']
    y_train = train_data['labels']
    
    # DoÄŸrulama verisi  
    with open(DATA_PATH_CORRECT + "valid.pickle", "rb") as f:
        valid_data = pickle.load(f)
    X_valid = valid_data['features']
    y_valid = valid_data['labels']
    
    # Test verisi
    with open(DATA_PATH_CORRECT + "test.pickle", "rb") as f:
        test_data = pickle.load(f)
    X_test = test_data['features']
    y_test = test_data['labels']
    
    print(f"EÄŸitim verisi: {X_train.shape} - {len(np.unique(y_train))} sÄ±nÄ±f")
    print(f"DoÄŸrulama verisi: {X_valid.shape}")
    print(f"Test verisi: {X_test.shape}")
    
    return X_train, y_train, X_valid, y_valid, X_test, y_test

def load_label_names():
    """SÄ±nÄ±f isimlerini yÃ¼kler"""
    import os
    
    # Dosya yollarÄ±nÄ± dene
    possible_paths = [
        "Trafic Signs Preprocssed data/",
        "./Trafic Signs Preprocssed data/", 
        "Trafic Signs Preprocssed data\\",
        "Traffic Signs Preprocessed data/",
        "./Traffic Signs Preprocessed data/", 
        "Traffic Signs Preprocessed data\\",
        ""
    ]
    
    for path in possible_paths:
        try:
            labels_df = pd.read_csv(path + "label_names.csv")
            print(f"âœ… label_names.csv bulundu: {path}")
            return labels_df['SignName'].values
        except:
            continue
    
    print("âš ï¸  label_names.csv bulunamadÄ±, varsayÄ±lan isimler kullanÄ±lÄ±yor...")
    return [f"Class_{i}" for i in range(43)]

def explore_dataset(X_train, y_train, class_names):
    """Veri setini keÅŸfet ve gÃ¶rselleÅŸtir"""
    print("\n=== VERÄ° SETÄ° ANALÄ°ZÄ° ===")
    
    # Temel bilgiler
    print(f"GÃ¶rÃ¼ntÃ¼ boyutu: {X_train.shape[1:]}")
    print(f"GÃ¶rÃ¼ntÃ¼ sayÄ±sÄ±: {X_train.shape[0]}")
    print(f"SÄ±nÄ±f sayÄ±sÄ±: {len(np.unique(y_train))}")
    print(f"Piksel deÄŸer aralÄ±ÄŸÄ±: {X_train.min():.2f} - {X_train.max():.2f}")
    
    # SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±
    unique, counts = np.unique(y_train, return_counts=True)
    
    plt.figure(figsize=(15, 5))
    plt.bar(unique, counts)
    plt.title('SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±')
    plt.xlabel('SÄ±nÄ±f ID')
    plt.ylabel('GÃ¶rÃ¼ntÃ¼ SayÄ±sÄ±')
    plt.xticks(range(0, len(unique), 5))
    plt.grid(True, alpha=0.3)
    plt.show()
    
    # En fazla ve en az Ã¶rnekli sÄ±nÄ±flar
    class_distribution = pd.DataFrame({
        'Class_ID': unique,
        'Class_Name': [class_names[i] if i < len(class_names) else f"Class_{i}" for i in unique],
        'Count': counts
    })
    
    print("\nEn fazla Ã¶rnekli 5 sÄ±nÄ±f:")
    print(class_distribution.nlargest(5, 'Count'))
    
    print("\nEn az Ã¶rnekli 5 sÄ±nÄ±f:")
    print(class_distribution.nsmallest(5, 'Count'))
    
    return class_distribution

def show_sample_images(X_train, y_train, class_names, samples_per_class=5):
    """Her sÄ±nÄ±ftan Ã¶rnek gÃ¶rÃ¼ntÃ¼ler gÃ¶ster"""
    unique_classes = np.unique(y_train)
    
    # Ä°lk 10 sÄ±nÄ±ftan Ã¶rnekler gÃ¶ster
    fig, axes = plt.subplots(2, 5, figsize=(20, 8))
    fig.suptitle('Ã–rnek Trafik Ä°ÅŸaretleri (Ä°lk 10 SÄ±nÄ±f)', fontsize=16)
    
    for i, class_id in enumerate(unique_classes[:10]):
        row = i // 5
        col = i % 5
        
        # Bu sÄ±nÄ±fa ait gÃ¶rÃ¼ntÃ¼lerden birini seÃ§
        class_indices = np.where(y_train == class_id)[0]
        sample_idx = class_indices[0]
        
        # GÃ¶rÃ¼ntÃ¼yÃ¼ gÃ¶ster
        axes[row, col].imshow(X_train[sample_idx])
        class_name = class_names[class_id] if class_id < len(class_names) else f"Class_{class_id}"
        axes[row, col].set_title(f"ID:{class_id}\n{class_name[:20]}...", fontsize=10)
        axes[row, col].axis('off')
    
    plt.tight_layout()
    plt.show()

def check_data_quality(X_train, y_train):
    """Veri kalitesini kontrol et"""
    print("\n=== VERÄ° KALÄ°TESÄ° KONTROLÃœ ===")
    
    # Eksik deÄŸerler
    print(f"Eksik deÄŸer var mÄ±? {np.isnan(X_train).any()}")
    
    # SÄ±fÄ±r deÄŸerli gÃ¶rÃ¼ntÃ¼ler
    zero_images = np.sum(np.all(X_train == 0, axis=(1,2,3)))
    print(f"Tamamen siyah gÃ¶rÃ¼ntÃ¼ sayÄ±sÄ±: {zero_images}")
    
    # Histogram analizi (parlaklik daÄŸÄ±lÄ±mÄ±)
    mean_brightness = np.mean(X_train, axis=(1,2,3))
    
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.hist(mean_brightness, bins=50, alpha=0.7)
    plt.title('Ortalama ParlaklÄ±k DaÄŸÄ±lÄ±mÄ±')
    plt.xlabel('Ortalama Piksel DeÄŸeri')
    plt.ylabel('Frekans')
    
    # Renk kanallarÄ± analizi
    plt.subplot(1, 2, 2)
    for channel, color in enumerate(['red', 'green', 'blue']):
        channel_mean = np.mean(X_train[:, :, :, channel])
        plt.bar(channel, channel_mean, color=color, alpha=0.7, label=f'{color.capitalize()}')
    
    plt.title('Ortalama Renk KanalÄ± DeÄŸerleri')
    plt.xlabel('Renk KanalÄ±')
    plt.ylabel('Ortalama DeÄŸer')
    plt.legend()
    
    plt.tight_layout()
    plt.show()

def normalize_data(X_train, X_valid, X_test):
    """Veriyi normalize et (0-1 arasÄ±na getir)"""
    print("Veri normalize ediliyor...")
    
    # EÄŸer veri 0-255 arasÄ±ndaysa normalize et
    if X_train.max() > 1:
        X_train_norm = X_train.astype('float32') / 255.0
        X_valid_norm = X_valid.astype('float32') / 255.0
        X_test_norm = X_test.astype('float32') / 255.0
        print(f"Normalizasyon tamamlandÄ±. Yeni aralÄ±k: {X_train_norm.min():.3f} - {X_train_norm.max():.3f}")
    else:
        X_train_norm = X_train.astype('float32')
        X_valid_norm = X_valid.astype('float32')
        X_test_norm = X_test.astype('float32')
        print("Veri zaten normalize edilmiÅŸ.")
    
    return X_train_norm, X_valid_norm, X_test_norm

# Ana kod
if __name__ == "__main__":
    print("ğŸš¦ TRAFÄ°K Ä°ÅARETÄ° SINIFLANDIRMA PROJESÄ°")
    print("=" * 50)
    
    # 1. Veriyi yÃ¼kle
    X_train, y_train, X_valid, y_valid, X_test, y_test = load_preprocessed_data()
    
    # 2. SÄ±nÄ±f isimlerini yÃ¼kle
    class_names = load_label_names()
    print(f"YÃ¼klenen sÄ±nÄ±f sayÄ±sÄ±: {len(class_names)}")
    
    # 3. Veriyi keÅŸfet
    class_distribution = explore_dataset(X_train, y_train, class_names)
    
    # 4. Ã–rnek gÃ¶rÃ¼ntÃ¼leri gÃ¶ster
    show_sample_images(X_train, y_train, class_names)
    
    # 5. Veri kalitesini kontrol et
    check_data_quality(X_train, y_train)
    
    # 6. Veriyi normalize et
    X_train_norm, X_valid_norm, X_test_norm = normalize_data(X_train, X_valid, X_test)
    
    print("\nâœ… Veri analizi tamamlandÄ±!")
    print("Sonraki adÄ±m: Model oluÅŸturma ve eÄŸitim")
    print("\nKullanÄ±m iÃ§in:")
    print("X_train_norm, y_train -> EÄŸitim verisi")
    print("X_valid_norm, y_valid -> DoÄŸrulama verisi") 
    print("X_test_norm, y_test -> Test verisi")
    print(f"class_names -> {len(class_names)} sÄ±nÄ±f ismi")