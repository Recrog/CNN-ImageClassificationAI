# Model Performans Analizi ve Raporu
# performance_evaluation.py

import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    classification_report, 
    confusion_matrix, 
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score
)
import tensorflow as tf
from tensorflow import keras
import cv2
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# TÃ¼rkÃ§e font ayarlarÄ±
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['font.size'] = 10

# Trafik iÅŸareti sÄ±nÄ±f isimleri
CLASS_NAMES = [
    'HÄ±z limiti (20km/h)', 'HÄ±z limiti (30km/h)', 'HÄ±z limiti (50km/h)', 'HÄ±z limiti (60km/h)',
    'HÄ±z limiti (70km/h)', 'HÄ±z limiti (80km/h)', 'HÄ±z limiti (80km/h) sonu', 'HÄ±z limiti (100km/h)',
    'HÄ±z limiti (120km/h)', 'GeÃ§me yasaÄŸÄ±', 'Kamyonlara geÃ§me yasaÄŸÄ±', 'KavÅŸak Ã¶ncelik yolu',
    'Ã–ncelikli yol', 'Yol ver', 'Dur', 'AraÃ§ trafiÄŸine kapalÄ±', 'Kamyonlara kapalÄ±', 'GiriÅŸ yasak',
    'Genel tehlike', 'Tehlikeli sol viraj', 'Tehlikeli saÄŸ viraj', 'Ã‡ifte viraj', 'Engebeli yol',
    'Kaygan yol', 'SaÄŸda daralan yol', 'Yol Ã§alÄ±ÅŸmasÄ±', 'Trafik Ä±ÅŸÄ±klarÄ±', 'Yaya geÃ§idi',
    'Ã‡ocuklar', 'Bisiklet geÃ§idi', 'Buzlu/karlÄ± yol', 'VahÅŸi hayvan geÃ§idi', 
    'HÄ±z ve geÃ§me yasaÄŸÄ± sonu', 'SaÄŸa dÃ¶nÃ¼ÅŸ zorunlu', 'Sola dÃ¶nÃ¼ÅŸ zorunlu', 'DÃ¼z gitme zorunlu',
    'DÃ¼z git veya saÄŸa dÃ¶n', 'DÃ¼z git veya sola dÃ¶n', 'SaÄŸdan geÃ§me zorunlu', 'Soldan geÃ§me zorunlu',
    'Ada saÄŸdan', 'Ada soldan', 'Zorunlu yÃ¶n'
]

def load_data_and_model():
    """Veri seti ve modeli yÃ¼kle"""
    print("ğŸ“Š Veri seti ve model yÃ¼kleniyor...")
    
    # Veri setini yÃ¼kle
    DATA_PATH = "Trafic Signs Preprocssed data/"
    
    # Test verisi
    with open(DATA_PATH + "test.pickle", "rb") as f:
        test_data = pickle.load(f)
    X_test = test_data['features']
    y_test = test_data['labels']
    
    # Veriyi normalize et
    if X_test.max() > 1:
        X_test = X_test.astype('float32') / 255.0
    else:
        X_test = X_test.astype('float32')
    
    # En son kaydedilen modeli bul
    model_files = [f for f in os.listdir('.') if f.endswith('.keras') or f.endswith('.h5')]
    if model_files:
        latest_model = max(model_files, key=os.path.getctime)
        model = tf.keras.models.load_model(latest_model)
        print(f"âœ… Model yÃ¼klendi: {latest_model}")
    else:
        raise FileNotFoundError("âŒ Model dosyasÄ± bulunamadÄ±!")
    
    print(f"âœ… Test verisi: {X_test.shape}")
    print(f"âœ… SÄ±nÄ±f sayÄ±sÄ±: {len(np.unique(y_test))}")
    
    return X_test, y_test, model, latest_model

def generate_predictions(model, X_test, y_test):
    """Model tahminlerini oluÅŸtur"""
    print("\nğŸ”® Tahminler oluÅŸturuluyor...")
    
    # Tahmin yap
    y_pred_proba = model.predict(X_test, verbose=1)
    y_pred = np.argmax(y_pred_proba, axis=1)
    
    print(f"âœ… {len(y_pred)} tahmin tamamlandÄ±")
    
    return y_pred, y_pred_proba

def calculate_basic_metrics(y_test, y_pred):
    """Temel metrikleri hesapla"""
    print("\nğŸ“ˆ Temel metrikler hesaplanÄ±yor...")
    
    # Temel metrikler
    accuracy = accuracy_score(y_test, y_pred)
    precision_macro = precision_score(y_test, y_pred, average='macro', zero_division=0)
    recall_macro = recall_score(y_test, y_pred, average='macro', zero_division=0)
    f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)
    
    precision_weighted = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    recall_weighted = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1_weighted = f1_score(y_test, y_pred, average='weighted', zero_division=0)
    
    metrics_dict = {
        'Accuracy': accuracy,
        'Precision (Macro)': precision_macro,
        'Recall (Macro)': recall_macro,
        'F1-Score (Macro)': f1_macro,
        'Precision (Weighted)': precision_weighted,
        'Recall (Weighted)': recall_weighted,
        'F1-Score (Weighted)': f1_weighted
    }
    
    return metrics_dict

def plot_confusion_matrix(y_test, y_pred, class_names, save_path=None):
    """Confusion Matrix'i gÃ¶rselleÅŸtir"""
    print("\nğŸ¯ Confusion Matrix oluÅŸturuluyor...")
    
    # Confusion matrix hesapla
    cm = confusion_matrix(y_test, y_pred)
    
    # YÃ¼zdelik confusion matrix
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
    
    # 2 ayrÄ± grafik: SayÄ± ve yÃ¼zde
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 10))
    
    # 1. SayÄ± bazÄ±nda
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=range(43), yticklabels=range(43), ax=ax1)
    ax1.set_title('Confusion Matrix (SayÄ±)', fontsize=16, fontweight='bold')
    ax1.set_xlabel('Tahmin Edilen SÄ±nÄ±f', fontsize=12)
    ax1.set_ylabel('GerÃ§ek SÄ±nÄ±f', fontsize=12)
    
    # 2. YÃ¼zde bazÄ±nda
    sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Oranges', 
                xticklabels=range(43), yticklabels=range(43), ax=ax2)
    ax2.set_title('Confusion Matrix (YÃ¼zde %)', fontsize=16, fontweight='bold')
    ax2.set_xlabel('Tahmin Edilen SÄ±nÄ±f', fontsize=12)
    ax2.set_ylabel('GerÃ§ek SÄ±nÄ±f', fontsize=12)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ’¾ Confusion matrix kaydedildi: {save_path}")
    
    plt.show()
    
    return cm

def analyze_class_performance(y_test, y_pred, class_names):
    """SÄ±nÄ±f bazÄ±nda performans analizi"""
    print("\nğŸ” SÄ±nÄ±f bazÄ±nda analiz yapÄ±lÄ±yor...")
    
    # Classification report
    report = classification_report(y_test, y_pred, 
                                 target_names=[f"Class_{i}" for i in range(43)],
                                 output_dict=True, zero_division=0)
    
    # DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼r
    df_report = pd.DataFrame(report).transpose()
    
    # SÄ±nÄ±f isimlerini ekle
    class_data = []
    for i in range(43):
        if f"Class_{i}" in df_report.index:
            class_data.append({
                'Class_ID': i,
                'Class_Name': class_names[i] if i < len(class_names) else f"Class_{i}",
                'Precision': df_report.loc[f"Class_{i}", 'precision'],
                'Recall': df_report.loc[f"Class_{i}", 'recall'],
                'F1-Score': df_report.loc[f"Class_{i}", 'f1-score'],
                'Support': int(df_report.loc[f"Class_{i}", 'support'])
            })
    
    df_class_performance = pd.DataFrame(class_data)
    
    return df_class_performance, report

def plot_class_performance(df_class_performance, save_path=None):
    """SÄ±nÄ±f performansÄ±nÄ± gÃ¶rselleÅŸtir"""
    print("\nğŸ“Š SÄ±nÄ±f performans grafikleri oluÅŸturuluyor...")
    
    fig, axes = plt.subplots(2, 2, figsize=(20, 12))
    
    # 1. F1-Score daÄŸÄ±lÄ±mÄ±
    axes[0,0].bar(df_class_performance['Class_ID'], df_class_performance['F1-Score'], 
                  color='skyblue', alpha=0.7)
    axes[0,0].set_title('SÄ±nÄ±f BazÄ±nda F1-Score', fontsize=14, fontweight='bold')
    axes[0,0].set_xlabel('SÄ±nÄ±f ID')
    axes[0,0].set_ylabel('F1-Score')
    axes[0,0].grid(True, alpha=0.3)
    axes[0,0].set_ylim(0, 1)
    
    # 2. Precision vs Recall
    scatter = axes[0,1].scatter(df_class_performance['Precision'], df_class_performance['Recall'], 
                               c=df_class_performance['F1-Score'], cmap='viridis', alpha=0.7, s=60)
    axes[0,1].set_title('Precision vs Recall', fontsize=14, fontweight='bold')
    axes[0,1].set_xlabel('Precision')
    axes[0,1].set_ylabel('Recall')
    axes[0,1].grid(True, alpha=0.3)
    plt.colorbar(scatter, ax=axes[0,1], label='F1-Score')
    
    # 3. Support (Ã¶rnek sayÄ±sÄ±) daÄŸÄ±lÄ±mÄ±
    axes[1,0].bar(df_class_performance['Class_ID'], df_class_performance['Support'], 
                  color='lightcoral', alpha=0.7)
    axes[1,0].set_title('SÄ±nÄ±f BazÄ±nda Test Ã–rneÄŸi SayÄ±sÄ±', fontsize=14, fontweight='bold')
    axes[1,0].set_xlabel('SÄ±nÄ±f ID')
    axes[1,0].set_ylabel('Ã–rnek SayÄ±sÄ±')
    axes[1,0].grid(True, alpha=0.3)
    
    # 4. En iyi ve en kÃ¶tÃ¼ performans gÃ¶steren sÄ±nÄ±flar
    top_5 = df_class_performance.nlargest(5, 'F1-Score')
    bottom_5 = df_class_performance.nsmallest(5, 'F1-Score')
    
    combined = pd.concat([top_5, bottom_5])
    colors = ['green']*5 + ['red']*5
    
    y_pos = range(len(combined))
    axes[1,1].barh(y_pos, combined['F1-Score'], color=colors, alpha=0.7)
    axes[1,1].set_yticks(y_pos)
    axes[1,1].set_yticklabels([f"{row['Class_ID']}: {row['Class_Name'][:20]}..." 
                               for _, row in combined.iterrows()], fontsize=8)
    axes[1,1].set_title('En Ä°yi ve En KÃ¶tÃ¼ Performans (F1-Score)', fontsize=14, fontweight='bold')
    axes[1,1].set_xlabel('F1-Score')
    axes[1,1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ’¾ Performans grafikleri kaydedildi: {save_path}")
    
    plt.show()

def create_performance_summary(metrics_dict, df_class_performance, model_name):
    """Performans Ã¶zet raporu oluÅŸtur"""
    print("\nğŸ“‹ Performans Ã¶zet raporu oluÅŸturuluyor...")
    
    # Genel statistikler
    avg_f1 = df_class_performance['F1-Score'].mean()
    std_f1 = df_class_performance['F1-Score'].std()
    min_f1 = df_class_performance['F1-Score'].min()
    max_f1 = df_class_performance['F1-Score'].max()
    
    # En iyi ve en kÃ¶tÃ¼ sÄ±nÄ±flar
    best_class = df_class_performance.loc[df_class_performance['F1-Score'].idxmax()]
    worst_class = df_class_performance.loc[df_class_performance['F1-Score'].idxmin()]
    
    # Rapor metni
    summary_report = f"""
ğŸ¯ TRAFIK Ä°ÅARETÄ° SINIFLAMA MODEL PERFORMANS RAPORU
{'='*70}

ğŸ“Š MODEL BÄ°LGÄ°LERÄ°:
â€¢ Model DosyasÄ±: {model_name}
â€¢ Test Tarihi: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
â€¢ Toplam SÄ±nÄ±f SayÄ±sÄ±: 43
â€¢ Test Ã–rnek SayÄ±sÄ±: {df_class_performance['Support'].sum()}

ğŸ“ˆ GENEL PERFORMANS METRÄ°KLERÄ°:
â€¢ Accuracy (DoÄŸruluk): {metrics_dict['Accuracy']:.4f} ({metrics_dict['Accuracy']*100:.2f}%)
â€¢ Precision (Macro): {metrics_dict['Precision (Macro)']:.4f} ({metrics_dict['Precision (Macro)']*100:.2f}%)
â€¢ Recall (Macro): {metrics_dict['Recall (Macro)']:.4f} ({metrics_dict['Recall (Macro)']*100:.2f}%)
â€¢ F1-Score (Macro): {metrics_dict['F1-Score (Macro)']:.4f} ({metrics_dict['F1-Score (Macro)']*100:.2f}%)
â€¢ F1-Score (Weighted): {metrics_dict['F1-Score (Weighted)']:.4f} ({metrics_dict['F1-Score (Weighted)']*100:.2f}%)

ğŸ“Š SINIF BAZINDA Ä°STATÄ°STÄ°KLER:
â€¢ Ortalama F1-Score: {avg_f1:.4f} Â± {std_f1:.4f}
â€¢ En YÃ¼ksek F1-Score: {max_f1:.4f}
â€¢ En DÃ¼ÅŸÃ¼k F1-Score: {min_f1:.4f}
â€¢ F1-Score AralÄ±ÄŸÄ±: {max_f1-min_f1:.4f}

ğŸ† EN Ä°YÄ° PERFORMANS GÃ–STEREN SINIF:
â€¢ SÄ±nÄ±f ID: {best_class['Class_ID']}
â€¢ SÄ±nÄ±f AdÄ±: {best_class['Class_Name']}
â€¢ F1-Score: {best_class['F1-Score']:.4f}
â€¢ Precision: {best_class['Precision']:.4f}
â€¢ Recall: {best_class['Recall']:.4f}

âš ï¸  EN DÃœÅÃœK PERFORMANS GÃ–STEREN SINIF:
â€¢ SÄ±nÄ±f ID: {worst_class['Class_ID']}
â€¢ SÄ±nÄ±f AdÄ±: {worst_class['Class_Name']}
â€¢ F1-Score: {worst_class['F1-Score']:.4f}
â€¢ Precision: {worst_class['Precision']:.4f}
â€¢ Recall: {worst_class['Recall']:.4f}

ğŸ¯ PERFORMANS DEÄERLENDÄ°RMESÄ°:
"""
    
    # Performans deÄŸerlendirmesi
    if metrics_dict['Accuracy'] >= 0.95:
        summary_report += "â€¢ âœ… Ã‡OK Ä°YÄ°: Model Ã§ok yÃ¼ksek doÄŸruluk gÃ¶steriyor (â‰¥95%)\n"
    elif metrics_dict['Accuracy'] >= 0.90:
        summary_report += "â€¢ âœ… Ä°YÄ°: Model yÃ¼ksek doÄŸruluk gÃ¶steriyor (90-95%)\n"
    elif metrics_dict['Accuracy'] >= 0.80:
        summary_report += "â€¢ âš¡ ORTA: Model kabul edilebilir doÄŸruluk gÃ¶steriyor (80-90%)\n"
    else:
        summary_report += "â€¢ âŒ DÃœÅÃœK: Model dÃ¼ÅŸÃ¼k doÄŸruluk gÃ¶steriyor (<80%)\n"
    
    if std_f1 < 0.1:
        summary_report += "â€¢ âœ… KARARLILILIK: SÄ±nÄ±flar arasÄ± performans tutarlÄ± (dÃ¼ÅŸÃ¼k std)\n"
    else:
        summary_report += "â€¢ âš ï¸ DAÄINIKLIK: BazÄ± sÄ±nÄ±flar dÃ¼ÅŸÃ¼k performans gÃ¶steriyor\n"
    
    summary_report += f"\nğŸ“Š RAPOR Ã–ZETÄ°:\n"
    summary_report += f"Bu model, 43 trafik iÅŸareti sÄ±nÄ±fÄ±nÄ± %{metrics_dict['Accuracy']*100:.1f} doÄŸrulukla\n"
    summary_report += f"sÄ±nÄ±flandÄ±rabilir. Genel F1-Score %{metrics_dict['F1-Score (Weighted)']*100:.1f} olup,\n"
    summary_report += f"pratik kullanÄ±m iÃ§in {'uygun' if metrics_dict['Accuracy'] >= 0.85 else 'iyileÅŸtirme gerektirir'}.\n"
    
    return summary_report

def save_detailed_report(df_class_performance, summary_report, model_name):
    """DetaylÄ± raporu dosyaya kaydet"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # CSV dosyasÄ±
    csv_filename = f"performance_report_{timestamp}.csv"
    df_class_performance.to_csv(csv_filename, index=False, encoding='utf-8-sig')
    
    # Text raporu
    txt_filename = f"summary_report_{timestamp}.txt"
    with open(txt_filename, 'w', encoding='utf-8') as f:
        f.write(summary_report)
    
    print(f"ğŸ’¾ DetaylÄ± rapor kaydedildi: {csv_filename}")
    print(f"ğŸ’¾ Ã–zet rapor kaydedildi: {txt_filename}")
    
    return csv_filename, txt_filename

def main_evaluation():
    """Ana deÄŸerlendirme fonksiyonu"""
    print("ğŸš¦ TRAFÄ°K Ä°ÅARETÄ° MODEL PERFORMANS ANALÄ°ZÄ°")
    print("="*60)
    
    try:
        # 1. Veri ve modeli yÃ¼kle
        X_test, y_test, model, model_name = load_data_and_model()
        
        # 2. Tahminleri oluÅŸtur
        y_pred, y_pred_proba = generate_predictions(model, X_test, y_test)
        
        # 3. Temel metrikleri hesapla
        metrics_dict = calculate_basic_metrics(y_test, y_pred)
        
        # 4. SonuÃ§larÄ± yazdÄ±r
        print("\nğŸ¯ GENEL PERFORMANS METRÄ°KLERÄ°:")
        print("-" * 40)
        for metric, value in metrics_dict.items():
            print(f"{metric:25}: {value:.4f} ({value*100:.2f}%)")
        
        # 5. Confusion Matrix
        cm = plot_confusion_matrix(y_test, y_pred, CLASS_NAMES, 
                                 save_path=f"confusion_matrix_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
        
        # 6. SÄ±nÄ±f bazÄ±nda analiz
        df_class_performance, report = analyze_class_performance(y_test, y_pred, CLASS_NAMES)
        
        # 7. Performans grafiklerini gÃ¶ster
        plot_class_performance(df_class_performance, 
                             save_path=f"class_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
        
        # 8. En iyi ve en kÃ¶tÃ¼ sÄ±nÄ±flarÄ± gÃ¶ster
        print("\nğŸ† EN Ä°YÄ° PERFORMANS GÃ–STEREN 5 SINIF (F1-Score):")
        print(df_class_performance.nlargest(5, 'F1-Score')[['Class_ID', 'Class_Name', 'F1-Score', 'Precision', 'Recall']])
        
        print("\nâš ï¸ EN DÃœÅÃœK PERFORMANS GÃ–STEREN 5 SINIF (F1-Score):")
        print(df_class_performance.nsmallest(5, 'F1-Score')[['Class_ID', 'Class_Name', 'F1-Score', 'Precision', 'Recall']])
        
        # 9. Ã–zet rapor oluÅŸtur
        summary_report = create_performance_summary(metrics_dict, df_class_performance, model_name)
        
        # 10. Raporu gÃ¶ster
        print(summary_report)
        
        # 11. RaporlarÄ± kaydet
        csv_file, txt_file = save_detailed_report(df_class_performance, summary_report, model_name)
        
        print(f"\nâœ… PERFORMANS ANALÄ°ZÄ° TAMAMLANDI!")
        print(f"ğŸ“Š DetaylÄ± veriler: {csv_file}")
        print(f"ğŸ“‹ Ã–zet rapor: {txt_file}")
        print(f"ğŸ¯ Confusion Matrix ve grafikler: PNG dosyalarÄ±")
        
        return df_class_performance, metrics_dict, summary_report
        
    except Exception as e:
        print(f"âŒ Hata oluÅŸtu: {str(e)}")
        print("LÃ¼tfen model ve veri dosyalarÄ±nÄ±n doÄŸru konumda olduÄŸundan emin olun.")

if __name__ == "__main__":
    # Performans analizini Ã§alÄ±ÅŸtÄ±r
    df_results, metrics, summary = main_evaluation()